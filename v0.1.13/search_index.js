var documenterSearchIndex = {"docs":
[{"location":"man/readtimesteps/#Read-timesteps-back-into-LaMEM","page":"Reading timesteps","title":"Read timesteps back into LaMEM","text":"","category":"section"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"If you want to quantitatively do something with the results, there is an easy way to read the output of a LaMEM timestep back into julia. All routines related to that are part of the LaMEM.IO module.","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> using LaMEM","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"You can first read the *.pvd file in the directory to see which timesteps are available:","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> FileName=\"FB_multigrid\"\njulia> DirName =\"test\"\njulia> Timestep, Filenames, Time = Read_LaMEM_simulation(FileName, DirName)\n([0, 1], [\"Timestep_00000000_0.00000000e+00/FB_multigrid.pvtr\", \"Timestep_00000001_6.72970343e+00/FB_multigrid.pvtr\"], [0.0, 6.729703])","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"We can read a particular timestep (say 1) with:","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> data, time = Read_LaMEM_timestep(FileName, 1, DirName)\n(CartData \n    size    : (33, 33, 33)\n    x       ϵ [ 0.0 : 1.0]\n    y       ϵ [ 0.0 : 1.0]\n    z       ϵ [ 0.0 : 1.0]\n    fields  : (:phase, :visc_total, :visc_creep, :velocity, :pressure, :strain_rate, :j2_dev_stress, :j2_strain_rate)\n  attributes: [\"note\"]\n, [6.729703])","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"The output is in a CartData structure (as defined in GeophysicalModelGenerator).","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"If you do not indicate a directory name (DirName) it'll look in your current directory. The default above will load the main LaMEM simulation output. Alternatively, you can also load the phase information by specify the optional keyword phase=true:","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> data, time = Read_LaMEM_timestep(FileName, 1, DirName, phase=true)\n(CartData \n    size    : (96, 96, 96)\n    x       ϵ [ 0.0052083334885537624 : 0.9947916269302368]\n    y       ϵ [ 0.0052083334885537624 : 0.9947916269302368]\n    z       ϵ [ 0.0052083334885537624 : 0.9947916269302368]\n    fields  : (:phase,)\n  attributes: [\"note\"]\n, [6.729703])","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"In the same way, you can load the internal free surface with surf=true (if that was saved), or passive tracers (passive_tracers=true). If you don't want to load all the fields in the file back to julia, you can check which fields are available:","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> Read_LaMEM_fieldnames(FileName, DirName)\n(\"phase [ ]\", \"visc_total [ ]\", \"visc_creep [ ]\", \"velocity [ ]\", \"pressure [ ]\", \"strain_rate [ ]\", \"j2_dev_stress [ ]\", \"j2_strain_rate [ ]\")","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"and load only part of those:","category":"page"},{"location":"man/readtimesteps/","page":"Reading timesteps","title":"Reading timesteps","text":"julia> data, time = Read_LaMEM_timestep(FileName, 1, DirName, fields=(\"phase [ ]\", \"visc_total [ ]\",\"velocity [ ]\"))\n(CartData \n    size    : (33, 33, 33)\n    x       ϵ [ 0.0 : 1.0]\n    y       ϵ [ 0.0 : 1.0]\n    z       ϵ [ 0.0 : 1.0]\n    fields  : (:phase, :visc_total, :velocity)\n  attributes: [\"note\"]\n, [6.729703])","category":"page"},{"location":"man/runlamem/#Run-LaMEM","page":"Run LaMEM","title":"Run LaMEM","text":"","category":"section"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"Go to the package manager & install it with:","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"julia>]\npkg>add LaMEM","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"It will automatically download a binary version of LaMEM which runs in parallel (along with the correct PETSc version). This will work on linux, mac and windows.","category":"page"},{"location":"man/runlamem/#Starting-a-simulation","page":"Run LaMEM","title":"Starting a simulation","text":"","category":"section"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"As usual, you need a LaMEM (*.dat) input file, which you can run in parallel (here on 4 cores) with:","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"julia> ParamFile=\"input_files/FallingBlock_Multigrid.dat\";\njulia> run_lamem(ParamFile, 4,\"-time_end 1\")\n-------------------------------------------------------------------------- \n                   Lithosphere and Mantle Evolution Model                   \n     Compiled: Date: Sep 10 2022 - Time: 06:21:30           \n-------------------------------------------------------------------------- \n        STAGGERED-GRID FINITE DIFFERENCE CANONICAL IMPLEMENTATION           \n-------------------------------------------------------------------------- \nParsing input file : input_files/FallingBlock_Multigrid.dat \n   Adding PETSc option: -snes_type ksponly\n   Adding PETSc option: -js_ksp_monitor\n   Adding PETSc option: -crs_pc_type bjacobi\nFinished parsing input file : input_files/FallingBlock_Multigrid.dat \n--------------------------------------------------------------------------\nTime stepping parameters:\n   Simulation end time          : 1. [ ] \n   Maximum number of steps      : 10 \n   Time step                    : 10. [ ] \n   Minimum time step            : 1e-05 [ ] \n   Maximum time step            : 100. [ ] \n   Time step increase factor    : 0.1 \n   CFL criterion                : 0.5 \n   CFLMAX (fixed time steps)    : 0.5 \n   Output time step             : 0.2 [ ] \n   Output every [n] steps       : 1 \n   Output [n] initial steps     : 1 \n--------------------------------------------------------------------------","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"The last parameter are optional PETSc command-line options. By default it runs on one processor.","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"Please note that you will have to be in the correct directory or indicate where that directory is. If you are in a different directory, the easiest way to change to the correct one is by using the changefolder function (on Windows and Mac):","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"julia> changefolder()","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"Alternatively, you can use the build-in terminal/shell in julia, which you can access with:","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"julia>;\nshell>cd ~/LaMEM/input_models/BuildInSetups/","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"use the Backspace key to return to the julia REPL.","category":"page"},{"location":"man/runlamem/","page":"Run LaMEM","title":"Run LaMEM","text":"Once you have performed a simulation, you can look at the results by opening the *.pvd files with Paraview. In this example, that would be FB_multigrid.pvd and FB_multigrid_phase.pvd.","category":"page"},{"location":"man/listfunctions/#List-of-all-functions","page":"List of functions","title":"List of all functions","text":"","category":"section"},{"location":"man/listfunctions/","page":"List of functions","title":"List of functions","text":"These are all functions that are available in the package, which can roughly be divided inton two groups (running & reading LaMEM)","category":"page"},{"location":"man/listfunctions/#Running-LaMEM","page":"List of functions","title":"Running LaMEM","text":"","category":"section"},{"location":"man/listfunctions/","page":"List of functions","title":"List of functions","text":"Modules = [LaMEM.Run]\n","category":"page"},{"location":"man/listfunctions/#LaMEM.Run.remove_popup_messages_mac-Tuple{}","page":"List of functions","title":"LaMEM.Run.remove_popup_messages_mac","text":"remove_popup_messages_mac()\n\nOn a Mac with firewall enabled, running LaMEM will result in a popup window that says: \"Accept incoming connections\" which you should Allow or Deny. This is a bit annoying, so this julia script fixes that. Note that you must have administrator rights on your machine as we need to run \"sudo\"\n\nRun this script from the terminal with\n\njulia> remove_popup_messages_mac()\n\nYou need to do this once (every time a new version is installed)\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.Run.run_lamem","page":"List of functions","title":"LaMEM.Run.run_lamem","text":"run_lamem(ParamFile::String, cores::Int64=1, args:String=\"\"; wait=true)\n\nThis starts a LaMEM simulation, for using the parameter file ParamFile on cores number of cores.  Optional additional command-line parameters can be specified with args.\n\nExample:\n\nYou can call LaMEM with:\n\njulia> using LaMEM\njulia> ParamFile=\"../../input_models/BuildInSetups/FallingBlock_Multigrid.dat\";\njulia> run_lamem(ParamFile)\n\nDo the same on 2 cores with a command-line argument as:\n\njulia> ParamFile=\"../../input_models/BuildInSetups/FallingBlock_Multigrid.dat\";\njulia> run_lamem(ParamFile, 2, \"-nstep_max = 1\")\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.Run.run_lamem_save_grid","page":"List of functions","title":"LaMEM.Run.run_lamem_save_grid","text":"ProcessorPartFile = run_lamem_save_grid(ParamFile::String, cores::Int64=1; verbose=true)\n\nThis calls LaMEM simulation, for using the parameter file ParamFile  and creates processor paritioning file \"ProcessorPartitioningcorescpuX.Y.Z.bin\" for cores number of cores. \n\nExample:\n\njulia> using LaMEM\njulia> ParamFile=\"../../input_models/BuildInSetups/FallingBlock_Multigrid.dat\";\njulia> ProcessorPartFile = run_lamem_save_grid(ParamFile, 2)\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.Run.show_paths_LaMEM-Tuple{}","page":"List of functions","title":"LaMEM.Run.show_paths_LaMEM","text":"show_paths_LaMEM()\n\nThe downloaded LaMEM binaries can also be called from outside julia (directly from the terminal).  In that case, you will need to set load correct dynamic libraries (such as PETSc) and call the correct binaries.\n\nThis function shows this for your system. \n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#Reading-LaMEM-output-back-into-julia","page":"List of functions","title":"Reading LaMEM output back into julia","text":"","category":"section"},{"location":"man/listfunctions/","page":"List of functions","title":"List of functions","text":"Modules = [LaMEM.IO]","category":"page"},{"location":"man/listfunctions/#LaMEM.IO.ReadField_3D_pVTR-Tuple{Any, Any}","page":"List of functions","title":"LaMEM.IO.ReadField_3D_pVTR","text":"output, isCell = ReadField_3D_pVTR(data, FieldName::String)\n\nExtracts a 3D data field from a pVTR data structure data\n\nInput:\n\ndata:       Data structure obtained with ReadVTRFile\nFieldName:  Exact name of the field as specified in the *.vtr file\n\nOutput:\n\ndata_field, isCell:   3D field with data, and a flag that indicates whether it is Cell data (PointData otherwise)                              data_field is a tuple of size 1, or 3 depending on whether it is a scalar or vector field\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.ReadField_3D_pVTU-Tuple{Any, Any}","page":"List of functions","title":"LaMEM.IO.ReadField_3D_pVTU","text":"output, isCell = ReadField_3D_pVTU(data, FieldName::String)\n\nExtracts a 3D data field from a pVTU data structure data Input:\n\ndata:       Data structure obtained with ReadVTRFile\nFieldName:  Exact name of the field as specified in the *.vtr file\n\nOutput:\n\ndata_field: Array with data, data_field is a tuple of size 1, 3 or 9 depending on whether it is a scalar, vector or tensor field\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_PVTR_File-Tuple{String, String}","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_PVTR_File","text":"data_output = Read_LaMEM_PVTR_File(DirName, FileName; fields=nothing)\n\nReads a 3D LaMEM timestep from VTR file FileName, located in directory DirName.  By default, it will read all fields. If you want you can only read a specific field. See the function fieldnames to get a list with all available fields in the file.\n\nIt will return data_output which is a CartData output structure.\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_PVTS_File-Tuple{String, String}","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_PVTS_File","text":"data_output = Read_LaMEM_PVTS_File(DirName, FileName; field=nothing)\n\nReads a 3D LaMEM timestep from VTS file FileName, located in directory DirName. Typically this is done to read passive tracers back into julia.  By default, it will read all fields. If you want you can only read a specific field. See the function fieldnames to get a list with all available fields in the file.\n\nIt will return data_output which is a CartData output structure.\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_PVTU_File-Tuple{Any, Any}","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_PVTU_File","text":"data_output = Read_LaMEM_PVTU_File(DirName, FileName; fields=nothing)\n\nReads a 3D LaMEM timestep from VTU file FileName, located in directory DirName. Typically this is done to read passive tracers back into julia.  By default, it will read all fields. If you want you can only read a specific field. See the function fieldnames to get a list with all available fields in the file.\n\nIt will return data_output which is a CartData output structure.\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_fieldnames","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_fieldnames","text":"Read_LaMEM_fieldnames(FileName::String, DirName_base::String=\"\"; phase=false, surf=false, tracers=false)\n\nReturns the names of the datasets stored in FileName\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_simulation","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_simulation","text":"Timestep, FileNames, Time = Read_LaMEM_simulation(FileName::String, DirName::String=\"\"; phase=false, surf=false, passive_tracers=false)\n\nReads a LaMEM simulation FileName in directory DirName and returns the timesteps, times and filenames of that simulation.\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.IO.Read_LaMEM_timestep","page":"List of functions","title":"LaMEM.IO.Read_LaMEM_timestep","text":"data, time = Read_LaMEM_timestep(FileName::String, TimeStep::Int64=0, DirName::String=\"\"; fields=nothing, phase=false, surf=false, last=false)\n\nThis reads a LaMEM timestep.\n\nInput Arguments:\n\nFileName: name of the simulation, w/out extension\nTimestep: timestep to be read, unless last=true in which case we read the last one\nDirName: name of the main directory (i.e. where the *.pvd files are located)\nfields: Tuple with optional fields; if not specified all will be loaded\nphase: Loads the phase information of LaMEM if true\nsurf: Loads the free surface of LaMEM if true\npassive_tracers: Loads passive tracers if true\nlast: Loads the last timestep\n\nOutput:\n\ndata: Cartesian data struct with LaMEM output\ntime: The time of the timestep\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.IO.changefolder-Tuple{}","page":"List of functions","title":"LaMEM.IO.changefolder","text":"changefolder()\n\nStarts a GUI on Windowss or Mac machines, which allows you to change our working directory\n\n\n\n\n\n","category":"method"},{"location":"man/listfunctions/#LaMEM.IO.clean_directory","page":"List of functions","title":"LaMEM.IO.clean_directory","text":"clean_directory(DirName)\n\nRemoves all LaMEM timesteps & *.pvd files from the directory DirName\n\n\n\n\n\n","category":"function"},{"location":"man/listfunctions/#LaMEM.IO.readPVD-Tuple{String}","page":"List of functions","title":"LaMEM.IO.readPVD","text":"FileNames, Time, Timestep = readPVD(FileName::String)\n\nThis reads a PVD file & returns the FileNames, Time and Timesteps\n\n\n\n\n\n","category":"method"},{"location":"man/installation_HPC/#Installation-on-HPC-systems","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"","category":"section"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Installing LaMEM on high performance computer (HPC) systems can be complicated, because you will have to compile PETSc with the correct dependencies for that system.  The reason is that HPC systems use MPI versions that are specifically tailored/compiled for that system. ","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Luckily there is a solution thanks to the great work of @eschnett and colleagues, who developed MPITrampoline which is an intermediate layer between the HPC-system-specific MPI libraries and the precompiled LaMEM binaries. ","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"It essentially consists of two steps:      1) compile a small package (MPIwrapper)      2) make sure that you download the version of MAGEMin that was compiled versus MPItrampoline.","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Here step-by-step instructions (for Linux, as that is what essentially all HPC systems use):","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Download MPIwrapper: ","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"$git clone https://github.com/eschnett/MPIwrapper.git \n$cd MPIwrapper","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Install it after making sure that mpiexec points to the one you want (you may have to load some modules, depending on your system):","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"$cmake -S . -B build -DMPIEXEC_EXECUTABLE=mpiexec -DCMAKE_BUILD_TYPE=RelWithDebInfo -DCMAKE_INSTALL_PREFIX=$HOME/mpiwrapper\n$cmake --build build\n$cmake --install build","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"At this stage, MPItrampoline is installed in $HOME/mpiwrapper","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Set the correct wrapper:","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"$export MPITRAMPOLINE_LIB=$HOME/mpiwrapper/lib64/libmpiwrapper.so","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Depending on the system it may be called lib instead of lib64 (check!).","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Start julia and install the MPI and MPIPreferences packages:","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"$julia\njulia> ]\npkg>add MPI, MPIPreferences","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Set the preference to use MPItrampoline","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"julia> using MPIPreferences; MPIPreferences.use_jll_binary(\"MPItrampoline_jll\")\n┌ Info: MPIPreferences unchanged\n└   binary = \"MPItrampoline_jll\"","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Load MPI and verify it is the correct one","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"julia> using MPI\njulia> MPI.Get_library_version()\n\"MPIwrapper 2.10.3, using MPIABI 2.9.0, wrapping:\\nOpen MPI v4.1.4, package: Open MPI boris@Pluton Distribution, ident: 4.1.4, repo rev: v4.1.4, May 26, 2022\"","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"After this, restart julia (this only needs to be done once, next time all is fine).","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"Now load LaMEM and check that it uses the mpitrampoline version:","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"julia> using MPI,LaMEM\njulia> LaMEM.LaMEM_jll.host_platform\nLinux x86_64 {cxxstring_abi=cxx11, julia_version=1.8.1, libc=glibc, libgfortran_version=5.0.0, mpi=mpitrampoline}","category":"page"},{"location":"man/installation_HPC/","page":"Installation on HPC systems","title":"Installation on HPC systems","text":"At this stage the precompiled version of LaMEM should be useable on that system.","category":"page"},{"location":"#LaMEM.jl","page":"Home","title":"LaMEM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is the julia interface to LaMEM, which does a number of handy things:","category":"page"},{"location":"","page":"Home","title":"Home","text":"It will automatically download a binary installation of LaMEM, along with the correct version of PETSc and mpiexec for your system. You can also use these binaries directly from your terminal, so you are not limited to julia. Gone are the days where you had to first spend hours or days to install PETSc on your system!\nWe provide a simple function to run LaMEM from julia (also in parallel).\nWe provide functions to read timesteps back into julia. ","category":"page"},{"location":"man/installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"Installing LaMEM can simply be done through the package manager:","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"julia>]\npkg>add LaMEM","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"which will download the binaries along with PETSc and mpiexec for your system.","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"You can test if it works on your machine with","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"pkg> test LaMEM","category":"page"},{"location":"man/installation/#Running-LaMEM-from-the-julia-REPL","page":"Installation","title":"Running LaMEM from the julia REPL","text":"","category":"section"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"Running LaMEM from within julia can be done with the run_lamem function:","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"LaMEM.run_lamem","category":"page"},{"location":"man/installation/#LaMEM.Run.run_lamem","page":"Installation","title":"LaMEM.Run.run_lamem","text":"run_lamem(ParamFile::String, cores::Int64=1, args:String=\"\"; wait=true)\n\nThis starts a LaMEM simulation, for using the parameter file ParamFile on cores number of cores.  Optional additional command-line parameters can be specified with args.\n\nExample:\n\nYou can call LaMEM with:\n\njulia> using LaMEM\njulia> ParamFile=\"../../input_models/BuildInSetups/FallingBlock_Multigrid.dat\";\njulia> run_lamem(ParamFile)\n\nDo the same on 2 cores with a command-line argument as:\n\njulia> ParamFile=\"../../input_models/BuildInSetups/FallingBlock_Multigrid.dat\";\njulia> run_lamem(ParamFile, 2, \"-nstep_max = 1\")\n\n\n\n\n\n","category":"function"},{"location":"man/installation/#Running-LaMEM-from-outside-julia","page":"Installation","title":"Running LaMEM from outside julia","text":"","category":"section"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"If you, for some reason, do not want to run LaMEM through julia but instead directly from the terminal or powershell, you will have to add the required dynamic libraries and executables. Do this with:","category":"page"},{"location":"man/installation/","page":"Installation","title":"Installation","text":"LaMEM.show_paths_LaMEM","category":"page"},{"location":"man/installation/#LaMEM.Run.show_paths_LaMEM","page":"Installation","title":"LaMEM.Run.show_paths_LaMEM","text":"show_paths_LaMEM()\n\nThe downloaded LaMEM binaries can also be called from outside julia (directly from the terminal).  In that case, you will need to set load correct dynamic libraries (such as PETSc) and call the correct binaries.\n\nThis function shows this for your system. \n\n\n\n\n\n","category":"function"}]
}
